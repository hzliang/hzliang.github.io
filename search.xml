<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Intriguing properties of neural networks]]></title>
    <url>%2F2018%2F03%2F12%2FIntriguing-properties-of-neural-networks%2F</url>
    <content type="text"><![CDATA[神经网络中两个有趣的特点 First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. 高层级的网络激活单元不包含语义，语义存在于网络激活单元的空间向量中：某一次层的自然坐标和随机选择的激活单元组合得到的样本都有语义相似性。 Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclas- sify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input. 对抗样本：对训练集做一些轻微的扰动，产生的样本，在训练好的模型上很可能得到错误的预测，并且这些样本即使是在使用不同的网络层数，参数，训练集训练得到的模型都会有这种现象。]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[难分样本挖掘]]></title>
    <url>%2F2018%2F03%2F12%2Fhard-negative-mining%2F</url>
    <content type="text"><![CDATA[Let’s say I give you a bunch of images that contain one or more people, and I give you bounding boxes for each one. Your classifier will need both positive training examples (person) and negative training examples (not person). For each person, you create a positive training example by looking inside that bounding box. But how do you create useful negative examples? A good way to start is to generate a bunch of random bounding boxes, and for each that doesn’t overlap with any of your positives, keep that new box as a negative. OK, so you have positives and negatives, so you train a classifier, and to test it out, you run it on your training images again with a sliding window. But it turns out that your classifier isn’t very good, because it throws a bunch of false positives (people detected where there aren’t actually people). A hard negative is when you take that falsely detected patch, and explicitly create a negative example out of that patch, and add that negative to your training set. When you retrain your classifier, it should perform better with this extra knowledge, and not make as many false positives. pyimagesearch Apply hard-negative mining. For each image and each possible scale of each image in your negative training set, apply the sliding window technique and slide your window across the image. At each window compute your HOG descriptors and apply your classifier. If your classifier (incorrectly) classifies a given window as an object (and it will, there will absolutely be false-positives), record the feature vector associated with the false-positive patch along with the probability of the classification. This approach is called hard-negative mining]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>Hard negative mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo next主题配置参考文档]]></title>
    <url>%2F2018%2F02%2F24%2Fhexo%2F</url>
    <content type="text"><![CDATA[1 2 3]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
